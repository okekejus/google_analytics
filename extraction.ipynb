{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72cead6",
   "metadata": {},
   "source": [
    "# Google Analytics Extractions\n",
    "- Extracts page info/overall info for CAO website.\n",
    "- To be run daily, with the most recent outputs replacing older ones. \n",
    "- Output will follow a \"(Type)_ Data _(Period)_YYYY-MM-DD.gzip\" format. \n",
    "    - \"Page_Data_Daily_2025-11-11.gzip\" for daily page extractions for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34455ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard modules \n",
    "import os \n",
    "import pandas as pd \n",
    "import sys \n",
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np \n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# Analytics modules\n",
    "from google.analytics.data_v1beta import BetaAnalyticsDataClient, Filter, FilterExpression, DateRange, Metric, Dimension, RunReportRequest\n",
    "from google.auth.transport.requests import Request \n",
    "import math\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f5457",
   "metadata": {},
   "source": [
    "## Fiscal Year/Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1374046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_end(current_date, end_date) -> bool: \n",
    "\n",
    "    \"\"\" Compares the current date to the last date of the fiscal year/quarter, used to determine the right end date to use for reports. \"\"\"\n",
    "\n",
    "    if (isinstance(current_date, date)&isinstance(end_date, date)): \n",
    "        val = current_date < end_date\n",
    "    else: \n",
    "        raise TypeError(\"Aruguments must be of type date.\")\n",
    "    return val\n",
    "\n",
    "def determine_fiscal_year(current_date) -> tuple: \n",
    "\n",
    "    \"\"\" Fetching Fiscal Year range based on the current date. \"\"\"\n",
    "    \n",
    "    if isinstance(current_date, date): \n",
    "        if current_date.month < 4: # months 1, 2, 3 are considered part of the last fiscal year\n",
    "            fiscal_start = datetime(current_date.year -1, 4, 1).date().strftime(\"%Y-%m-%d\")\n",
    "            fiscal_end = datetime(current_date.year, 3, 31).date()\n",
    "        else: # everything else will use the current year \n",
    "            fiscal_start = datetime(current_date.year, 4, 1).date().strftime(\"%Y-%m-%d\")\n",
    "            fiscal_end = datetime(current_date.year+1, 3, 31).date()\n",
    "\n",
    "        if compare_to_end(current_date, fiscal_end): \n",
    "            fiscal_end = current_date.strftime(\"%Y-%m-%d\")\n",
    "        else: \n",
    "            fiscal_end = fiscal_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    else: \n",
    "        raise TypeError(\"Argument must be of type date.\")\n",
    "    \n",
    "    return (fiscal_start, fiscal_end)\n",
    "\n",
    "def determine_fiscal_quarter(current_date) -> tuple: \n",
    "\n",
    "    \"\"\"Fetching Quarter range based on the current date\"\"\"\n",
    "\n",
    "    if isinstance(current_date, date): \n",
    "        cao_quarters = {\"Q1\": [4,5,6], \n",
    "                    \"Q2\": [7,8,9], \n",
    "                    \"Q3\": [10,11,12],\n",
    "                    \"Q4\": [1,2,3]}\n",
    "        if (current_date.month in [d for d in cao_quarters.get(\"Q1\")]): \n",
    "            quarter_start = datetime(current_date.year, 4,1).date().strftime(\"%Y-%m-%d\")\n",
    "            quarter_end = datetime(current_date.year, 6, 30).date()\n",
    "        elif (current_date.month in [d for d in cao_quarters.get(\"Q2\")]):\n",
    "            quarter_start = datetime(current_date.year, 7,1).date().strftime(\"%Y-%m-%d\")\n",
    "            quarter_end = datetime(current_date.year, 9, 30).date()\n",
    "        elif (current_date.month in [d for d in cao_quarters.get(\"Q3\")]):\n",
    "            quarter_start = datetime(current_date.year, 10,1).date().strftime(\"%Y-%m-%d\")\n",
    "            quarter_end = datetime(current_date.year, 12, 31).date()\n",
    "        else: \n",
    "            quarter_start = datetime(current_date.year, 1,1).date().strftime(\"%Y-%m-%d\")\n",
    "            quarter_end = datetime(current_date.year, 3, 31).date()\n",
    "    else: \n",
    "        raise TypeError(\"Arguments must be of type date.\")\n",
    "    \n",
    "    \n",
    "    if compare_to_end(current_date, quarter_end): \n",
    "        quarter_end = current_date.strftime(\"%Y-%m-%d\")\n",
    "    else: \n",
    "        quarter_end = quarter_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return (quarter_start, quarter_end)\n",
    "\n",
    "# Pages individually\n",
    "def fetch_metrics_per_page(property_id, start_date, run_date):\n",
    "    with requests.Session() as session:    \n",
    "        \n",
    "        \n",
    "        client = BetaAnalyticsDataClient()\n",
    "        date_range = DateRange(start_date=start_date, end_date=run_date)\n",
    "        limit = 250000\n",
    "        offset = 0 #initialize counter for rows \n",
    "\n",
    "        metrics = [Metric(name='screenPageViews'),\n",
    "                   Metric(name=\"newUsers\"), \n",
    "                   Metric(name=\"totalUsers\"),\n",
    "                   Metric(name=\"screenPageViewsPerUser\"), \n",
    "                   Metric(name=\"screenPageViewsPerSession\"), \n",
    "                   Metric(name=\"userEngagementDuration\")]\n",
    "        dimensions = [Dimension(name=\"unifiedPagePathScreen\")]\n",
    "\n",
    "\n",
    "        # submit request for report based on metrics and dimensions, specifying limit and offset for pagination\n",
    "        request = RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "        date_ranges=[date_range],\n",
    "        metrics=metrics,\n",
    "        dimensions=dimensions,\n",
    "        limit = limit, \n",
    "        offset = 0)\n",
    "        # Run the report\n",
    "        response = client.run_report(request)\n",
    "\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for row in response.rows:\n",
    "            row_data = {}\n",
    "            for i, dimension in enumerate(row.dimension_values):\n",
    "                row_data[dimensions[i].name] = dimension.value\n",
    "            for i, metric in enumerate(row.metric_values):\n",
    "                row_data[metrics[i].name] = metric.value\n",
    "            data.append(row_data)\n",
    "\n",
    "        total_rows = response.row_count \n",
    "\n",
    "        if total_rows > limit: \n",
    "            addon = []\n",
    "            reps = ((total_rows - limit)/limit)\n",
    "            iterations = math.ceil(reps)\n",
    "\n",
    "            for i in tqdm(range(iterations), desc=\"Downloading...\"): # strip the tqdm function if you'd like --> for i in range(iterations):\n",
    "                offset = offset + limit\n",
    "\n",
    "                request = RunReportRequest(\n",
    "                property=f\"properties/{property_id}\",\n",
    "                date_ranges=[date_range],\n",
    "                metrics=metrics,\n",
    "                dimensions=dimensions,\n",
    "                limit = limit, \n",
    "                offset = offset)\n",
    "\n",
    "                response = client.run_report(request)\n",
    "\n",
    "                for row in response.rows:\n",
    "                    row_data = {}\n",
    "                    for i, dimension in enumerate(row.dimension_values):\n",
    "                        row_data[dimensions[i].name] = dimension.value\n",
    "                    for i, metric in enumerate(row.metric_values):\n",
    "                        row_data[metrics[i].name] = metric.value\n",
    "                    addon.append(row_data)\n",
    "\n",
    "            results_df = pd.concat([pd.DataFrame(data), pd.DataFrame(addon)])\n",
    "        else: \n",
    "            results_df = pd.DataFrame(data)\n",
    "        session.close()\n",
    "        results_df[\"date\"] = start_date\n",
    "        return results_df\n",
    "            \n",
    "\n",
    "# fetch aggregated data\n",
    "def fetch_metrics_aggregate(property_id, start_date, run_date):\n",
    "    with requests.Session() as session:    \n",
    "        \n",
    "        client = BetaAnalyticsDataClient()\n",
    "        date_range = DateRange(start_date=start_date, end_date=run_date)\n",
    "        limit = 250000\n",
    "        offset = 0\n",
    "\n",
    "        metrics = [\n",
    "            Metric(name='screenPageViews'),\n",
    "            Metric(name='newUsers'),\n",
    "            Metric(name=\"totalUsers\"),\n",
    "            Metric(name='screenPageViewsPerUser'),\n",
    "            Metric(name='screenPageViewsPerSession'),\n",
    "            Metric(name='userEngagementDuration')\n",
    "        ]\n",
    "\n",
    "        # Initial request\n",
    "        request = RunReportRequest(\n",
    "            property=f\"properties/{property_id}\",\n",
    "            date_ranges=[date_range],\n",
    "            metrics=metrics,\n",
    "            limit=limit,\n",
    "            offset=offset\n",
    "        )\n",
    "\n",
    "        response = client.run_report(request)\n",
    "\n",
    "        # Convert rows to list of dicts\n",
    "        data = []\n",
    "        for row in response.rows:\n",
    "            d = {m.name: float(v.value) for m, v in zip(response.metric_headers, row.metric_values)}\n",
    "            d[\"date\"] = start_date\n",
    "            data.append(d)\n",
    "\n",
    "        total_rows = response.row_count\n",
    "\n",
    "        # Handle pagination if needed\n",
    "        if total_rows > limit:\n",
    "            addon = []\n",
    "            iterations = math.ceil((total_rows - limit) / limit)\n",
    "\n",
    "            for i in tqdm(range(iterations), desc=\"Downloading...\"):\n",
    "                offset += limit\n",
    "                request = RunReportRequest(\n",
    "                    property=f\"properties/{property_id}\",\n",
    "                    date_ranges=[date_range],\n",
    "                    metrics=metrics,\n",
    "                    limit=limit,\n",
    "                    offset=offset\n",
    "                )\n",
    "                response = client.run_report(request)\n",
    "\n",
    "                for row in response.rows:\n",
    "                    d = {m.name: float(v.value) for m, v in zip(response.metric_headers, row.metric_values)}\n",
    "                    d[\"date\"] = start_date\n",
    "                    addon.append(d)\n",
    "\n",
    "            results_df = pd.concat([pd.DataFrame(data), pd.DataFrame(addon)], ignore_index=True)\n",
    "        else:\n",
    "            results_df = pd.DataFrame(data)\n",
    "\n",
    "        session.close()\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0bf6",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39436887",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().date()\n",
    "\n",
    "previous_day = (current_date - timedelta(1))\n",
    "\n",
    "fiscal_quarter = determine_fiscal_quarter(current_date)\n",
    "\n",
    "fiscal_year = determine_fiscal_year(current_date)\n",
    "property_id = \"PROPERTY_ID\"\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"APPLICATION_CREDENTIALS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946d22d",
   "metadata": {},
   "source": [
    "# Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424eabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual pages \n",
    "daily_all_pages = fetch_metrics_per_page(property_id, previous_day.strftime(\"%Y-%m-%d\"), previous_day.strftime(\"%Y-%m-%d\"))\n",
    "daily_all_pages.to_parquet(f\"ga_reports/page_data/daily/Page_Data_Daily_{previous_day}.gzip\", index=False) # export to gzip\n",
    "\n",
    "# aggregated pages\n",
    "daily_overall = fetch_metrics_aggregate(property_id, previous_day.strftime(\"%Y-%m-%d\"), previous_day.strftime(\"%Y-%m-%d\"))\n",
    "daily_overall.to_parquet(f\"ga_reports/aggregate_data/daily/Overall_Data_Daily_{previous_day}.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3caf3",
   "metadata": {},
   "source": [
    "# Monthly Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date prep\n",
    "month_last_day = calendar.monthrange(current_date.year, current_date.month)[1]\n",
    "\n",
    "month_start = f\"{current_date.year}-{current_date.month}-01\"\n",
    "\n",
    "month_end = str()\n",
    "\n",
    "if compare_to_end(current_date, datetime(current_date.year,current_date.month,month_last_day).date()): \n",
    "    month_end = current_date.strftime(\"%Y-%m-%d\")\n",
    "else: \n",
    "    month_end = f\"{current_date.year}-{current_date.month}-{month_last_day}\"\n",
    "\n",
    "# individual pages \n",
    "monthly_all_pages = fetch_metrics_per_page(property_id, month_start, month_end)\n",
    "\n",
    "monthly_all_pages.to_parquet(f\"ga_reports/page_data/monthly/Page_Data_Monthly_{month_start}.gzip\", index=False) # export to gzip\n",
    "\n",
    "# Common issues \n",
    "commonissues = pd.read_excel(\"ga_reports/reference_files/MostCommonIssues.xlsx\")\n",
    "\n",
    "a3_links = pd.read_excel(\"MostCommonIssues.xlsx\", sheet_name=\"3A\").dropna()\n",
    "b3_links = pd.read_excel(\"MostCommonIssues.xlsx\", sheet_name=\"3B\").dropna()\n",
    "\n",
    "a3_links[\"type\"] = \"3A\"\n",
    "b3_links[\"type\"] = \"3B\"\n",
    "\n",
    "a3_links.columns = [\"unifiedPagePathScreen\", \"type\"]\n",
    "b3_links.columns = [\"unifiedPagePathScreen\", \"type\"]\n",
    "\n",
    "most_common_issues = monthly_all_pages[monthly_all_pages.unifiedPagePathScreen.isin(commonissues.link)].sort_values(\"unifiedPagePathScreen\")\n",
    "\n",
    "data_frames = [most_common_issues, a3_links, b3_links]\n",
    "\n",
    "most_common_issues = reduce(lambda left, right: pd.merge(left, right, on=[\"unifiedPagePathScreen\"], how=\"left\"), data_frames) # fusing all reports based on link \n",
    "\n",
    "most_common_issues[\"type\"] = most_common_issues[\"type_x\"].replace(pd.NA, \"\") + most_common_issues[\"type_y\"].replace(pd.NA, \"\")\n",
    "\n",
    "most_common_issues = most_common_issues.drop([\"type_x\", \"type_y\"], axis=1) # getting rid of unnecessary columns \n",
    "\n",
    "\n",
    "other_issues = pd.read_excel(\"MostCommonIssues.xlsx\", sheet_name=\"OtherIssues\") # read in issues that are not considered most common, repeat the process\n",
    "\n",
    "a3_links = pd.read_excel(\"MostCommonIssues.xlsx\", sheet_name=\"3A\").dropna()\n",
    "b3_links = pd.read_excel(\"MostCommonIssues.xlsx\", sheet_name=\"3B\").dropna()\n",
    "\n",
    "a3_links[\"type\"] = \"3A\"\n",
    "b3_links[\"type\"] = \"3B\"\n",
    "\n",
    "a3_links.columns = [\"unifiedPagePathScreen\", \"type\"]\n",
    "b3_links.columns = [\"unifiedPagePathScreen\", \"type\"]\n",
    "\n",
    "other_common_issues = monthly_all_pages[monthly_all_pages.unifiedPagePathScreen.isin(other_issues.link)].sort_values(\"unifiedPagePathScreen\")\n",
    "\n",
    "data_frames = [other_common_issues, a3_links, b3_links]\n",
    "\n",
    "other_common_issues = reduce(lambda left, right: pd.merge(left, right, on=[\"unifiedPagePathScreen\"], how=\"left\"), data_frames) # fusing all reports based on link \n",
    "\n",
    "other_common_issues[\"type\"] = other_common_issues[\"type_x\"].replace(pd.NA, \"\") + other_common_issues[\"type_y\"].replace(pd.NA, \"\")\n",
    "\n",
    "other_common_issues = other_common_issues.drop([\"type_x\", \"type_y\"], axis=1) # getting rid of unnecessary columns \n",
    "most_common_issues[\"issueType\"] = \"Common\"\n",
    "other_common_issues[\"issueType\"] = \"Uncommon\"\n",
    "all_common_issues = pd.concat([most_common_issues,other_common_issues]) #combination of all \n",
    "\n",
    "all_common_issues.to_parquet(f\"ga_reports/guided_steps_data/CommonIssues_Data_Monthly_{month_start}.gzip\", index=False) # export \n",
    "\n",
    "# Overall\n",
    "monthly_overall = fetch_metrics_aggregate(property_id, month_start, month_end)\n",
    "monthly_overall.to_parquet(f\"ga_reports/aggregate_data/monthly/Overall_Data_Monthly_{month_start}.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bd10c",
   "metadata": {},
   "source": [
    "# Weekly Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_of_week = current_date - timedelta(days=current_date.weekday()+1)\n",
    "end_of_week = start_of_week + timedelta(days = 6)\n",
    "start_of_week = start_of_week.strftime(\"%Y-%m-%d\")\n",
    "if compare_to_end(current_date, end_of_week): \n",
    "    end_of_week = current_date.strftime(\"%Y-%m-%d\")\n",
    "else: \n",
    "    end_of_week = end_of_week.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# individual\n",
    "weekly_pages = fetch_metrics_per_page(property_id, start_of_week, end_of_week)\n",
    "weekly_pages.to_parquet(f\"ga_reports/page_data/weekly/Page_Data_Weekly_{start_of_week}.gzip\", index=False)\n",
    "\n",
    "# Aggregate\n",
    "weekly_overall = fetch_metrics_aggregate(property_id, start_of_week, end_of_week)\n",
    "weekly_overall.to_parquet(f\"ga_reports/aggregate_data/weekly/Overall_Data_Weekly_{start_of_week}.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa0294",
   "metadata": {},
   "source": [
    "# Quarterly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de00ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual\n",
    "quarterly_pages = fetch_metrics_per_page(property_id, fiscal_quarter[0], fiscal_quarter[1])\n",
    "quarterly_pages.to_parquet(f\"ga_reports/page_data/quarterly/Pages_Data_Quarterly_{fiscal_quarter[0]}.gzip\", index=False)\n",
    "\n",
    "# aggregate\n",
    "quarterly_overall = fetch_metrics_aggregate(property_id, fiscal_quarter[0], fiscal_quarter[1])\n",
    "quarterly_overall.to_parquet(f\"ga_reports/aggregate_data/quarterly/Overall_Data_Quarterly_{fiscal_quarter[0]}.gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6b876",
   "metadata": {},
   "source": [
    "# Annual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual pages \n",
    "annual_pages = fetch_metrics_per_page(property_id, fiscal_year[0], fiscal_year[1])\n",
    "annual_pages.to_parquet(f\"ga_reports/page_data/annually/Pages_Data_Annual_{fiscal_year[0]}.gzip\", index=False)\n",
    "\n",
    "# overall page\n",
    "annual_overall = fetch_metrics_aggregate(property_id, fiscal_year[0], fiscal_year[1])\n",
    "annual_overall .to_parquet(f\"ga_reports/aggregate_data/annually/Overall_Data_Annual_{fiscal_year[0]}.gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
